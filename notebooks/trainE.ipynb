{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import sys, os, glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "\n",
    "from DGXutils import GetFileNames, GetLowestGPU\n",
    "from importlib import reload\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.dataset import *\n",
    "from utils.model import *\n",
    "from utils.GetLR import get_lr\n",
    "\n",
    "device = GetLowestGPU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "e = E_Dataset(split=\"train\", data_path = \"../data/maize_data_2014-2022_vs_2023_v2/\")\n",
    "e_train, e_val = random_split(e, [int(len(e)*0.8), len(e)-int(len(e)*0.8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GxE_Transformer(config=TransformerConfig, g_enc=False).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up optimizers, loss function, and scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "log_path = '../logs/e_model/log.txt'\n",
    "chckpnt_path = '../checkpoints/e_model/checkpoint_{0}.pt'\n",
    "\n",
    "# dataloaders\n",
    "# shuffle dataloaders\n",
    "train_loader = DataLoader(\n",
    "    e_train, \n",
    "    batch_size=64, \n",
    "    shuffle=True,\n",
    "    num_workers=64,\n",
    "    pin_memory=True)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    e_val, \n",
    "    batch_size=64, \n",
    "    shuffle=True,\n",
    "    num_workers=64,\n",
    "    pin_memory=True)\n",
    "\n",
    "# epochs\n",
    "num_epochs = 100\n",
    "batches_per_epoch = len(train_loader)\n",
    "num_iters = num_epochs * batches_per_epoch\n",
    "\n",
    "# other options\n",
    "batches_per_eval = len(val_loader)\n",
    "warmup_iters = batches_per_epoch\n",
    "lr_decay_iters = num_iters\n",
    "max_lr = 1e-3\n",
    "min_lr = 1e-7\n",
    "max_iters = num_iters\n",
    "log_interval = 1\n",
    "eval_interval = batches_per_epoch\n",
    "early_stop = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Epoch: 14 | Best Loss: 4.4302e+00\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "867c0281bc1a40978557fcead26a608c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# non-customizable options\n",
    "iter_update = 'train loss {1:.4e}, val loss {2:.4e}\\r'\n",
    "best_val_loss = None # initialize best validation loss\n",
    "last_improved = 0 # start early stopping counter\n",
    "iter_num = 0 # initialize iteration counter\n",
    "epoch_num = 0 # initialize epoch counter\n",
    "t0 = time.time() # start timer\n",
    "\n",
    "# training loop\n",
    "# refresh log\n",
    "with open(log_path, 'w') as f: \n",
    "    f.write(f'epoch,train_loss,val_loss\\n')\n",
    "\n",
    "# keep training until break\n",
    "while True:\n",
    "\n",
    "    # clear print output\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    if best_val_loss is not None:\n",
    "        print('---------------------------------------\\n',\n",
    "            f'Epoch: {epoch_num} | Best Loss: {best_val_loss:.4e}\\n', \n",
    "            '---------------------------------------', sep = '')\n",
    "    else:\n",
    "        print('-------------\\n',\n",
    "            f'Epoch: {epoch_num}\\n', \n",
    "            '-------------', sep = '')\n",
    "\n",
    "    # ----------\n",
    "    # checkpoint\n",
    "    # ----------\n",
    "\n",
    "    # estimate loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss, val_loss = 0, 0\n",
    "        with tqdm(total=batches_per_eval, desc='Eval') as pbar:\n",
    "            for (xbt, ybt), (xbv, ybv) in zip(train_loader, val_loader):\n",
    "\n",
    "                # send to device\n",
    "                for key, value in xbt.items():\n",
    "                    xbt[key] = torch.nan_to_num(value, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                    xbt[key] = value.to(device)\n",
    "                ybt = ybt.to(device)\n",
    "                for key, value in xbv.items():\n",
    "                    xbv[key] = torch.nan_to_num(value, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                    xbv[key] = value.to(device)\n",
    "                ybv = ybv.to(device)\n",
    "\n",
    "                train_loss += loss_function(model(xbt), ybt).item()\n",
    "                val_loss += loss_function(model(xbv), ybv).item()\n",
    "                pbar.update(1)\n",
    "                if pbar.n == pbar.total:\n",
    "                    break\n",
    "        train_loss /= batches_per_eval\n",
    "        val_loss /= batches_per_eval\n",
    "    model.train()\n",
    "\n",
    "    # update user\n",
    "    print(iter_update.format(epoch_num, train_loss, val_loss)) \n",
    "\n",
    "    # update log\n",
    "    with open(log_path, 'a') as f: \n",
    "        f.write(f'{epoch_num},{train_loss},{val_loss}\\n')\n",
    "\n",
    "    # checkpoint model\n",
    "    if iter_num > 0:\n",
    "        checkpoint = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': epoch_num,\n",
    "            'best_val_loss': best_val_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, chckpnt_path.format(epoch_num))\n",
    "\n",
    "    # book keeping\n",
    "    if best_val_loss is None:\n",
    "        best_val_loss = val_loss\n",
    "\n",
    "    if epoch_num > 0:\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            last_improved = 0\n",
    "            print(f'*** validation loss improved: {best_val_loss:.4e} ***')\n",
    "        else:\n",
    "            last_improved += 1\n",
    "            print(f'validation has not improved in {last_improved} epochs')\n",
    "        if last_improved > (early_stop - 1):\n",
    "            print()\n",
    "            print(f'*** no improvement for {early_stop} epochs, stopping ***')\n",
    "            break\n",
    "\n",
    "    # --------\n",
    "    # backprop\n",
    "    # --------\n",
    "\n",
    "    # iterate over batches\n",
    "    with tqdm(total=eval_interval, desc='Train') as pbar:\n",
    "        for xb, yb in train_loader:\n",
    "\n",
    "            # update the model\n",
    "            for key, value in xb.items():\n",
    "                xb[key] = torch.nan_to_num(value, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                xb[key] = value.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            loss = loss_function(model(xb), yb)\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                print('loss is NaN, stopping')\n",
    "                break\n",
    "            \n",
    "            # apply learning rate schedule\n",
    "            lr = get_lr(it = iter_num,\n",
    "                        warmup_iters = warmup_iters, \n",
    "                        lr_decay_iters = lr_decay_iters, \n",
    "                        max_lr = max_lr, \n",
    "                        min_lr = min_lr)\n",
    "            \n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # update book keeping\n",
    "            pbar.update(1)\n",
    "            iter_num += 1\n",
    "            if iter_num % batches_per_epoch == 0:\n",
    "                epoch_num += 1\n",
    "            if pbar.n == pbar.total:\n",
    "                break\n",
    "\n",
    "    # break once hitting max_iters\n",
    "    if iter_num > max_iters:\n",
    "        print(f'maximum epochs reached: {num_epochs}')\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gp-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
