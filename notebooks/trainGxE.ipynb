{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import sys, os, glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from DGXutils import GetFileNames, GetLowestGPU\n",
    "from importlib import reload\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.dataset import *\n",
    "from utils.model import *\n",
    "from utils.GetLR import get_lr\n",
    "\n",
    "device = GetLowestGPU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "base_path = '../data/position_ec_raw_genotype/'\n",
    "\n",
    "gxe_train = GxE_Dataset(base_path + 'X_train.csv')\n",
    "gxe_val = GxE_Dataset(base_path + 'X_val.csv')\n",
    "gxe_test = GxE_Dataset(base_path + 'X_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataloaders\n",
    "train_loader = DataLoader(gxe_train, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(gxe_val, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = next(iter(train_loader))\n",
    "for key, value in xb.items():\n",
    "    xb[key] = value.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GxE_Transformer(config=TransformerConfig).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up optimizers, loss function, and scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other options\n",
    "batch_size = 32\n",
    "batches_per_eval = 1000\n",
    "warmup_iters = 1000\n",
    "lr_decay_iters = 120000\n",
    "max_lr = 1e-3\n",
    "min_lr = 1e-5\n",
    "max_iters = 150000\n",
    "log_interval = 1\n",
    "eval_interval = 1000\n",
    "early_stop = 50\n",
    "n_workers = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-customizable options\n",
    "iter_update = 'train loss {1:.4e}, val loss {2:.4e}\\r'\n",
    "best_val_loss = None # initialize best validation loss\n",
    "last_improved = 0 # start early stopping counter\n",
    "iter_num = 0 # initialize iteration counter\n",
    "t0 = time.time() # start timer\n",
    "\n",
    "# training loop\n",
    "# refresh log\n",
    "with open(log_path, 'w') as f: \n",
    "    f.write(f'iter_num,train_loss,val_loss\\n')\n",
    "\n",
    "# keep training until break\n",
    "while True:\n",
    "\n",
    "    # clear print output\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    if best_val_loss is not None:\n",
    "        print('---------------------------------------\\n',\n",
    "            f'Iteration: {iter_num} | Best Loss: {best_val_loss:.4e}\\n', \n",
    "            '---------------------------------------', sep = '')\n",
    "    else:\n",
    "        print('-------------\\n',\n",
    "            f'Iteration: {iter_num}\\n', \n",
    "            '-------------', sep = '')\n",
    "\n",
    "    #\n",
    "    # checkpoint\n",
    "    #\n",
    "\n",
    "    # shuffle dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_generator, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=n_workers,\n",
    "        pin_memory=True)\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_generator, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=n_workers,\n",
    "        pin_memory=True)\n",
    "\n",
    "    # estimate loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss, val_loss = 0, 0\n",
    "        with tqdm(total=batches_per_eval, desc=' Eval') as pbar:\n",
    "            for (xbt, ybt), (xbv, ybv) in zip(train_loader, val_loader):\n",
    "                xbt, ybt = xbt.to(device), ybt.to(device)\n",
    "                xbv, ybv = xbv.to(device), ybv.to(device)\n",
    "                train_loss += loss_function(model(xbt), ybt).item()\n",
    "                val_loss += loss_function(model(xbv), ybv).item()\n",
    "                pbar.update(1)\n",
    "                if pbar.n == pbar.total:\n",
    "                    break\n",
    "        train_loss /= batches_per_eval\n",
    "        val_loss /= batches_per_eval\n",
    "    model.train()\n",
    "\n",
    "    # update user\n",
    "    print(iter_update.format(iter_num, train_loss, val_loss)) \n",
    "\n",
    "    # update log\n",
    "    with open(log_path, 'a') as f: \n",
    "        f.write(f'{iter_num},{train_loss},{val_loss}\\n')\n",
    "\n",
    "    # checkpoint model\n",
    "    if iter_num > 0:\n",
    "        checkpoint = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'kwargs': model_kwargs,\n",
    "            'iter_num': iter_num,\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'train_ids': train_idx,\n",
    "            'val_ids': val_idx,\n",
    "        }\n",
    "        torch.save(checkpoint, chckpnt_path.format(iter_num))\n",
    "\n",
    "    # book keeping\n",
    "    if best_val_loss is None:\n",
    "        best_val_loss = val_loss\n",
    "\n",
    "    if iter_num > 0:\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            last_improved = 0\n",
    "            print(f'*** validation loss improved: {best_val_loss:.4e} ***')\n",
    "        else:\n",
    "            last_improved += 1\n",
    "            print(f'validation has not improved in {last_improved} steps')\n",
    "        if last_improved > early_stop:\n",
    "            print()\n",
    "            print(f'*** no improvement for {early_stop} steps, stopping ***')\n",
    "            break\n",
    "\n",
    "    # --------\n",
    "    # backprop\n",
    "    # --------\n",
    "\n",
    "    # shuffle dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_generator, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=n_workers,\n",
    "        pin_memory=True)\n",
    "\n",
    "    # iterate over batches\n",
    "    with tqdm(total=eval_interval, desc='Train') as pbar:\n",
    "        for xb, yb in train_loader:\n",
    "\n",
    "            # update the model\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "            loss = loss_function(model(xb), yb)\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                print('loss is NaN, stopping')\n",
    "                break\n",
    "            \n",
    "            # apply learning rate schedule\n",
    "            lr = get_lr(it = iter_num,\n",
    "                        warmup_iters = warmup_iters, \n",
    "                        lr_decay_iters = lr_decay_iters, \n",
    "                        max_lr = max_lr, \n",
    "                        min_lr = min_lr)\n",
    "            \n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # update book keeping\n",
    "            pbar.update(1)\n",
    "            iter_num += 1\n",
    "            if pbar.n == pbar.total:\n",
    "                break\n",
    "\n",
    "    # break once hitting max_iters\n",
    "    if iter_num > max_iters:\n",
    "        print(f'maximum iterations reached: {max_iters}')\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gp-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
